\section{Discussion}
\label{sec:discussion}
%
%
Several points of our lens proposal are worth discussing, as follows.

\vspace{0.15cm}
\noindent\textbf{Lens activation:} For our lens to be effective, the user needs to perceive at least a \emph{part} of the target in a normal DVR image. Obviously, if the target is fully occluded, either due to transfer function or viewport settings, this is not possible. However, we argue that in such cases, no other visualization tool can help the user find and explore such a target, since its presence is completely absent in the rendered image. Conversely, when at least a fragment of the target is visible, no matter how small, the user can point the lens at that fragment, after which the lens' mechanism performs the de-occlusion. Hence, the usage pipeline we support works as follows: (1) The user changes the viewpoint and/or transfer function until she sees some fragment of the target of interest; (2) The user points the lens at that fragment, without needing to change the viewpoint or the transfer function, and fully reveals it; (3) Optionally, the user adjusts the lens parameters to better examine the target.


\vspace{0.15cm}
\noindent\textbf{Lens shape:} Occluders are pushed away, and deformed, isotropically (Secs.~\ref{sec:scattering}, \ref{continuity}). The advantage of this simple lens model is that it requires a single parameter, the lens radius $R$, which makes its usage easy. The deformations evolve smoothly from the lens center (maximal) to outside the lens (no deformation), see Sec.~\ref{continuity}, which effectively blends the local (in lens) focus with the global (out of lens) contexts (R3). However, a side-effect is that this mechanism compresses the deformed occluders strongly close to the lens border, making them hardly visible when the lens is fully active. A possible refinement would be to reduce the deformation of the pushed-away occluders while still pushing them away, thereby improving the focus+context effect (R3). However, this would occlude the areas outside the lens with these undeformed, but pushed-away, occluders, so it will basically shift occlusion from \emph{inside} the lens to \emph{outside} and close to it. Finding an optimal balance between minimal deformation (so one can recognize the pushed-away occluders) and minimal clutter (so these occluders do not destroy the lens context) is an interesting topic for future work.

\vspace{0.15cm}
\noindent\textbf{Parameter setting:} Our lens depends on several parameters: the 2D lens center $\mathbf{f}$, lens radius $R$, lens axis direction $\mathbf{a}$, local light direction $\mathbf{l}^{lens}$, scattering start-distance $t_{min}$, and gathering and scattering parameters $\alpha$ and $\beta$. As explained in Sec.~\ref{sec:principle}, all these parameters can be easily controlled via a mouse-driven virtual trackball, key modifiers, and the arrow keys. While this seems complex at first sight, performing such operations is in fact quite simple, since the lens works at interactive frame rates (15 frames per second), so the user can quickly tune the parameters and see their effect. Moreover, all parameters start with typically good preset values (see Sec.~\ref{sec:principle} for details). A possible refinement would be to pre-segment the target of interest, based on user-given values for $\mathbf{f}$, $R$, and $t_{min}$, thereby determining $\beta$ automatically. This would help the first requirement
 (R1) which consists in giving a fast unobstructed view of the target. However, even if this were present, we believe that manual control of the scattering $\beta$ is important to allow users to choose their most suitable field-of-view angle. In fact, this flexibility allows a better exploration of the local context (R2).

\vspace{0.15cm}
\noindent\textbf{Implementation:} The entire lens is implemented by modifying the ray trajectories constructed in the inner loop (per-pixel raycasting) of a typical DVR raycaster\,\cite{cudasdk}. Apart from this modification, we also change the per-voxel lighting function and transfer function based on the voxel location with respect to the lens and the parameters supplied by user interaction (Sec.~\ref{sec:inter_expl}. As mentioned, such changes are limited and should be easily applicable to any raycaster. Since these changes work in a per-voxel or per-ray fashion, they are directly applicable to raycasters which parallelize computations for different ray sets. 

\vspace{0.15cm}
\noindent\textbf{Limitations:} Several limitations of our approach must be mentioned too. First and foremost, as explained, we cannot de-occlude a target that the user is not aware of (does not partially see) from any viewpoint. However, we argue that such a use-case is not in scope for a lens. At a higher level, many lens mechanisms exist in the literature, as discussed in Sec.~\ref{sec:related_work}. While we have argued that, to our knowledge, none of them simultaneously supports requirements R1,$\ldots$,R4, comparing such mechanisms with our lens for specific use-cases and datasets is still an important test for the end-to-end effectiveness of our proposal. We have not covered this point as obtaining several implementations of such lenses in the same interactive framework or tool, for comparison fairness, is very challenging. However, this remains an important open point for future work -- both for our proposal but also for all other volumetric lens proposals in the literature. Related to this, we have performed three user evaluations involving specialists in airport baggage security (Sec.~\ref{sec:baggage}), pulmonology (Sec.~\ref{sec:chest}), and air traffic control (Sec.~\ref{sec:atc}). In all cases, the users were not involved in this work, nor with other work of the authors. However, the set-up of these evaluations stays at the level of formative user experiments. To confirm and refine the obtained (positive) findings, more formal user studies are needed, which we plan to cover next.
